{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Feature Engineering Udemy Notes (强推).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBiHzlxhzNFIJR3yatq9rR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/ml-algo-implementation/blob/main/ML_Feature_Engineering_Udemy_Notes_(%E5%BC%BA%E6%8E%A8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEuceFumk-Ol"
      },
      "source": [
        "# Feature Engineeering\n",
        "\n",
        "\n",
        "Udemy Course: [Feature Engineering for ML](https://cisco.udemy.com/course/feature-engineering-for-machine-learning/learn/lecture/15675348#overview)\n",
        "\n",
        "1. Variable Types\n",
        "  - Numerical variable\n",
        "    - Discrete (eg. Number of bank account, number of pets in family)\n",
        "    \n",
        "    - Continuous (eg. amount money paid in customer each time, interest rate paid)\n",
        "      \n",
        "  - Categorical variable\n",
        "    - Ordinal (顺序有意义 eg. students grade: A, B, C..., Education degree: BA, MA... )\n",
        "    - Nominal (顺序无意义 eg. vehicle make: BMW, Mercedez, County of Birth: China, Germany)\n",
        "      - High Cardinality effect\n",
        "        - 某个categorical feature出现的label种类太多了，几乎每个record有不同的label (例如ID), tree-based model 对这样的feature importance束手无策 (over fitting)\n",
        "        - 如果要使用sciket-learn，就必须要将string转化为numerical，一定要encoding, 初级一点的one-hot encoding\n",
        "        - Uneven distribution btn training and teting dataset, 有些label只存在于训练集，有些则只存在于测试集\n",
        "        - Overfitting, 太多的label会导致overfitting，特别是tree based algo, 因为增加了split\n",
        "        - 可以通过target encoding解决，或者直接删掉这一个feature\n",
        "      - Rare lebels effect\n",
        "        - 类似high cardinality, 就是某些feature存在那么几个rare label，数量特别少，可以通过查看每个label的比例发现，会产生类似的问题，uneven啦，overfitting 啦，两个解决方法：\n",
        "          1. one-hot encoding \n",
        "          2. 全部直接标记为rare group\n",
        "  - Date and time\n",
        "    - 提取week of year, quater of year, how long since xxx 来作为特征\n",
        "    - 例如 Payment date ('1987-09-01 15:20:20'),可以derive：\n",
        "      - Day: 周几，是否是weekend\n",
        "      - Month：1-12，什么quarter, semster, 是否是holiday season\n",
        "      - Year：哪一年\n",
        "      - Hour: **上午下午白天晚上？**\n",
        "      - Elapsed time: 和第一次payment的time difference\n",
        "\n",
        "  - Mix variable (既有numerical也有categorical，eg. vehicle registration 湘A 22331) \n",
        "    - 一般会提取numerical和categorical features，放入Num和Cat两列\n",
        "    - number of missed payments (1-3, D, A)\n",
        "      - variable中又有数字又有category，就创建两列，一列放数字，一列放category，其余的都放missing\n",
        "\n",
        "2. Missing Data Imputation\n",
        "  - 直接删除record，有些时候data missing completely at random，missing records总数不超过5%， 这时候简单粗暴，直接删除 \n",
        "  - 直接删除feature，如果某个feature的missing value数太多，例如超过一半，也就没有留下来的必要了\n",
        "  - Missing data impuration 都是基于一个小假设：all data are misssing completely at random.\n",
        "  - Numerical Features \n",
        "    - 注意一个细节，填充都是用training data的值去替代training和test两个集合的NA\n",
        "    - mean/median imputation  (少量missing value的情况下)\n",
        "      - 填充完比较一下前后的data describe/variance\n",
        "    - arbitrary value imputation\n",
        "      - 直接填充正常range之外的outlier，例如`999`或`-1`完事儿\n",
        "      - 填充完比较一下前后的data describe/variance\n",
        "    - end of trail imputation\n",
        "      - 填充`mean+3*std`\n",
        "  - **Categorical Features (少量missing value的情况下)**\n",
        "    - freq category imputation\n",
        "     - Mode imputation, 直接填充mode value\n",
        "     - 这样在提取特征的时候其实很蛋疼,因为与事实不符\n",
        "    - add a 'missing' category\n",
        "      - 直接填充'missing'\n",
        "  - Both\n",
        "    - Random sample imputation\n",
        "      - 直接填充random values，但是这种方法很蛋疼的是可能每次训练的模型都不同，给出的prediction也不同，我们可以set eeed，虽然叫是叫是random，其实也没那么random\n",
        "    - Add a missing indicator + mean/median/mode imputation\n",
        "      - 在使用各种值填充的同时，加一列missing column表示是否`missing`，**特别是当missing value present in more than 5% of the observations。**\n",
        "3. Categorical Data Encoding\n",
        "  - 注意，所谓的categorical encoding谈论的都是一个column里面的不同label\n",
        "  - Traditional techniques\n",
        "    1. one-hot encoding / dummy variable\n",
        "      - 例如我们在color列中有red, green, blue等feature，直接每个颜色弄一列，要么1要么0，简单粗暴\n",
        "      - **当心dummy variablee trap,只需要为(k-1)个feature进行hot-encoding**\n",
        "      - 但是也有例外，例如tree-based models不需要做到(k-1),因为**tree model并不会用上所有的features**，还有当需要知道每个feature的importance的时候，也要用到k个variabless\n",
        "      - 优点是特别适合linear model，且保留了所有categorical values的信息，缺点是可能引起curse of dimensionality，太多redundant information\n",
        "    2. ordinal/label encoding\n",
        "      - 直接把categorical feature用數字1,2,3..代替\n",
        "      - 优点是简单粗暴，**和tree based model配合良好**，不会增加feature space，缺点是不适合linear model，而且好像给每个label排了序，其实并没有区别\n",
        "    3. count/freq encoding\n",
        "      - 用每个label的count/freq代表每个label，例如red出现了两次，red的label就是2\n",
        "      - 优点是简单粗暴，和tree based model配合良好，不会增加feature space，缺点是给不同的label一样的编码，lose valuable info，不适合linear model，不能handle new features\n",
        "  - Monotonic relationship\n",
        "    1. Target guided ordered label encoding\n",
        "      - 根据每个label对应target的平均个数的顺序来定label，属于monotonic relationship btn categories and target\n",
        "      - 优点是适用于linear和non-linear model，但是缺点是可能引起overfitting，有点难cross-validation\n",
        "    2. **mean/target encoding**\n",
        "      - 根据每个label对应target的平均个数的来定label, 详见[案例](https://maxhalford.github.io/blog/target-encoding/), 属于monotonic relationship btn categories and target\n",
        "      - 优点是适用于linear和non-linear model，**很适合high cardinality features**, 但是缺点是可能引起overfitting，有点难cross-validation，也可能出现不同label，相同编码\n",
        "    3. probability ratio encoding\n",
        "    4. weight of evidence (WOE)\n",
        "      - 公式：\n",
        "      WOE = ln([proportion of good events]/[proportion of bad events])\n",
        "      - WOE刚开始是出现在financial & credit industry用于衡量risk\n",
        "       of loan default, **这其实就是个odds**\n",
        "      - 优点是创造了天然的logistic scale，**特别适合logistic regression**，而且不同label之间WOE值可以相互比较，创造了一种label和target之间的monotonic relationship，缺点是还是容易产生overfitting，而且可能出现denominator为0的情况\n",
        "\n",
        "    5. 针对predominant variable/few categories/high cardinality三种情况\n",
        "      - predominant variable: 就两种label，一种label极少，另一种label 99%\n",
        "      - few categories:: 就几种label，一种label 99%，另几种label加起来也不多，直接encoding为rare label\n",
        "      - high cardinality: 使用targer encoding，或者把几种label全部加起来，直接encoding为rare label\n",
        "  - Altrenative techniques\n",
        "    1. binary eencoding\n",
        "      - 用二进制编码features\n",
        "    2. feature hashing\n",
        "      - 稀奇古怪的编码方法，一般竞赛可能用的多\n",
        "    3. others\n",
        "4. Linear Model Asssumption   **四大假设**\n",
        "  1. Linear relationshsip between variable and target\n",
        "    - Y = C + a0x0 + a1x1 + ... + anxn\n",
        "    - assess with scatter plots\n",
        "    - non-linear transformations of variables can improve the linear relationship\n",
        "    - **Residuals (difference btn predictions and real values) should be normaly distributed and center around zero**  \n",
        "  2. Multivariate normality \n",
        "    -  Each variable values follows Gausian disstribution\n",
        "    -  assess with Q-Q plot, residual plot and stat tested by komogrov-smirnov test\n",
        "    - log-transformation may help if variables are not normaly distributed \n",
        "  3. no or little co-linearity btn features\n",
        "    - assess with correlation matrix or the variance inflation factor (VIF)\n",
        "  4. Homoscedacity (of variance)\n",
        "    - the independent variables have the same finite variance, which means error terms is the same across all independent variables\n",
        "    - tested by \n",
        "       - residual plot \n",
        "       - levene's test\n",
        "       - barlett's test\n",
        "       - goldfeld-quandt's test\n",
        "    - non-linear transformaton and feature scailing can help improve homoscedacity\n",
        "5. Distributions\n",
        "  - Distribution is the likelihood of obtaining a certain value \n",
        "  - Linear model对数据的distribution要求较高，别的model，**例如svm，neural network 则不需要数据满足distribution，但是一般来说满足gaussian distribution会让模型表现的好一点**。下面列出改变distribution的一些方法：\n",
        "    1. log, $ln(x)$\n",
        "    2. Exponential, $X^{(n)}$\n",
        "    3. Reciprol, $1/x$\n",
        "    4. Box-cox， $\\frac{x^{\\lambda} - 1}{\\lambda}, \\lambda \\in[-5, 5]$\n",
        "  - Types\n",
        "    - Discrete\n",
        "      - Binomial\n",
        "      - Possion\n",
        "    - Continuous \n",
        "      - Gaussian\n",
        "      - Skewed\n",
        "        - 对于feature with skewed distribution， 我们采用median进行missing data imputation，因为此时median相比mean更能represent distribution\n",
        "6. Outliers\n",
        "  - 一般指value偏大或者偏小，超出population的正常范围很多\n",
        "  - 对不同的model影响不同\n",
        "    - 深受其害：linear model, regression-basd model, adaboost\n",
        "    - 影响不大：tree-based algo, Decision Tree or a Bagging Tree or a Random Forest, all these models can handle outliers very effectively.\n",
        "  - outlier检测：\n",
        "    1. normal distribution: 3 std, 5th or 95th quantile\n",
        "    2. skewed disstribution: IQR = 75th Quantile - 25th Quantile, Upper limite = 75th Quantile + 1.5 IQR, Lower limit = 25th Quantile + 1.5 IQR, 这个1.5可以增加到3\n",
        "      - 也可以直接看box-plot\n",
        "   - outlier处理：\n",
        "    1. trimming\n",
        "      - 直接删除\n",
        "      - 优点是简单粗暴，缺点是可能会是删除太多data\n",
        "    2. missing data\n",
        "      - 标记为missing data，按照missing data imputation处理\n",
        "    3. discretisation\n",
        "      - binning中归为upper或者lower bin\n",
        "    4. censoring\n",
        "      - capping，winsorization，top/bottom coding\n",
        "      - 用capping值替换，不允许大于/小于某个值\n",
        "        - capping种类：3倍std，75th quantile + 1.5 IQR / 25th quantile - 1.5IQR \n",
        "    \n",
        "\n",
        "\n",
        "7. Variable Transformation\n",
        "  - 将skewed数据分布变成normal distribution，一般会得到更好的模型表现\n",
        "  - 具体方法有：\n",
        "    1. logarithmic    \n",
        "      - `np.log(1+x)`\n",
        "    2. exponential/power tranformation    \n",
        "      - `x^{1/2}, x^{1/1.5}, x^{2}, x^{n}`都可以试试\n",
        "    3. reciprocal     \n",
        "      -  `1/x`\n",
        "    4. rank tranformation\n",
        "    5. box-cox\n",
        "      - `stat.boxcox(x)`   这里面的lambda是超参数，存在最优lambd\n",
        "    6. yeo-johnson  超复杂\n",
        "      - `stat.yeojohnson(x)`   这里面的lambda同样是超参数，存在最优lambda\n",
        "\n",
        "8. Discretisation 离散化\n",
        "  - **我们的对象都是continous numeric variables**，将continous numeric variables离散化成discrete numeric variables，也就是我们常说的**binning**，优点有：\n",
        "      1. 可以改变feature的distribution，将skewed变成unskewed\n",
        "      2. 可以处理outlier\n",
        "  - 具体方法分为supervised和unsupervised：\n",
        "    - Unsupervised\n",
        "      1. equal-width\n",
        "        - 平均分组，每组的间隔相同\n",
        "        - 优点是基本复制了原数列distribution，能够handle outliers，简单易行创建discrete values，易于下一步进行categorical encoding\n",
        "      2. equal-freq\n",
        "        - 分出n个组，每个组的variable数目大致相同\n",
        "        - 优点是improve value spread，distribution从尖峰变平了，也能够handle outliers，create discrete variables，也易于下一步进行categorical encoding\n",
        "      3. K means\n",
        "         - 在序列中找出n个centroid，离每个centroid较近的variabls成一组\n",
        "         - 优点是基本复制了原数列distribution，能够handle outliers (尽管outliers还是有影响，特别是最后一个bucket)，简单易行创建discrete values，易于下一步进行categorical encoding，缺点是是并木有提升distribution\n",
        "    - Supervised\n",
        "      1. decision tree\n",
        "        - 利用单个continous variable column和target的关系建立一个很简单(max_depth = 3)的decision tree，然后用每组的probability来进行binning\n",
        "        - 优点是能够handle outliers (tree based model 特性), 创建discrete values，更关键的是建立了monotonic relationship. \n",
        "    - Domain knowlwdge / Arbitrary \n",
        "      - 按照常识或者规定进行binning\n",
        "        - 例如20-40， 40-60一个年龄段，0-40k, 45k-60k一个收入段，直接人为定义。\n",
        "   - 当我们discretisation了之后，我们的continous variabels其实就变成了ordinal categorical feature，例如，第一组，第二组...或者[0, 4], [5, 10]这样的区间，我们又需要参照上面的categorical encoding进行转换\n",
        "\n",
        "9. Feature scailing \n",
        "  - 通常是feature processing pipeline最后一步，也就是马上要开始投入模型了\n",
        "  - features with bigger magnitude dominates the regression model\n",
        "  - feature scaling对model的影响不同：\n",
        "    - 深受其害：linear & logistic regression，Neural network, SVM, euclean distances based models (KNN & Kmeans), Linear Discriminant Analuysis (LDA), Principle Component Analysis (PCA)， Gradient Descent\n",
        "    - 影响不大：Tree-based model, Radome forest, Gradient boosted trees\n",
        "  - Feature scailing的方法\n",
        "    1. standardization\n",
        "      - z-score = (x-mean)/std\n",
        "      - 调整了feature的range，variance为1, 中心在0点，但是基本不改变distribution，outlier该在还是在，如果要transfor成normalization，不应该采用这样的方法，而应该用上面提到的variable transformation的方法\n",
        "    2. mean normalization\n",
        "      - x_scaled = (x-mean)/(max - min)\n",
        "      - 调整了feature的range，range为[-1,1], 中心在0点，可能会改变distribution，outlier该在还是在，如果要transfor成normalization，不应该采用这样的方法，而应该用上面提到的variable transformation的方法\n",
        "    3. scailing to max and min (MinMaXScailing)\n",
        "      - x_scaled = (x-min)/(max - min)\n",
        "      - 调整了feature的range，range为[0,1], variance varies，mean varies，可能会改变distribution，outlier该在还是在，如果要transfor成normalization，不应该采用这样的方法，而应该用上面提到的variable transformation的方法\n",
        "    4. scailing to absolute max (MaxAbsScailing)\n",
        "      - x_scaled = x/max(x)\n",
        "      - 感觉和MinMax效果差不多\n",
        "    5. scailing to median and quantiles (robust scailing)\n",
        "      - x_scaled = [x-median(x)]/[75th quant(x) - 25th quant(x)]\n",
        "      - median centred at 0, 之所以叫做robust scailing，就是因为用的是median和quantile，都是绝佳对抗outlier的工具，所以可以handle outliers\n",
        "    6. scailing to unit form\n",
        "      - 用多列的variable，创建L2 Norm，然后每个variable\n",
        "      除以L2 Norm\n",
        "      - 很特别的方法，不知道有啥特别之处\n",
        "\n",
        "\n",
        "\n",
        "9. Feature selsection\n",
        "  - **Note:** numerical feature我们常常可以通过binning转变为categorical feature，然后使用下列的方法\n",
        "  - Correlation method (compare btn only features)\n",
        "    - pearson's correlation coefficient (linear correlation)\n",
        "      - 大于0.7就算非常correlated了\n",
        "    - spearman's rank correlation coefficient\n",
        "    - kendall rank correlation coefficient\n",
        "  - Statistical method (compare btn features and target rank then select, higher rarnk or lower p-value)\n",
        "    - mutual information / information gain \n",
        "      - mutual information btn each feature and target, 越大越好\n",
        "    - Chi-quare / fisher score \n",
        "      - 多用于categorical features, target也是categorical，也就是classification，例如：想知道male/female这样的feature对是否survived有没有影响\n",
        "      - Null hypo为这个feature对target的预测无效，P-value越大，越能拒绝Null hypo，说明**相比之下**, 这个feature越important.\n",
        "    - univariate testss / ANOVA\n",
        "      - ANOVA的Null hypo是，两个或多个sample有着相同的mean。ANOVA的假设有：samples are independent, normaly distributed, 还有homogenity of variance\n",
        "\n",
        "  - Performance metric method （推荐，更现代，更适合machine learning）\n",
        "    - univariate roc-auc / rmse\n",
        "      - **用每个单独的feature例如用tree model做classification，按照每个feature的metric排序, 大于0.5的roc-auc一般都予以考虑。当你有大量的features\n",
        "      时候，这种方法比较好**\n",
        "        - classification: roc-auc, accuracy, precision, recall, etc\n",
        "        - regression: MSE, RMSE, R2, etc\n",
        "     - select features by target mean encoding\n",
        "        - 这种方法专门针对于numerical feature，先得到利用training set的target encoding，然后算出roc-auc进行ranking\n",
        "  - Coefficient method (并不推荐，因为assumption要求高，且regularization可能distort)\n",
        "    - Linear model / Logistic model \n",
        "      - 当满足下列assumption，那么coefficient系数就能代表feature的importance：\n",
        "        1. linear relationship \n",
        "        2. no multicoliniearity\n",
        "        3. independent\n",
        "        4. normally distributed\n",
        "        5. **all features in the same scale**, needs feature scailing\n",
        "      - 如果采用了regularization，coeeficient的大小会收到影响，就不适用了\n",
        "  - Laaso-regularization\n",
        "    - Regularization的意义在于reduce the freedom of the model, hence the model will be less likely to fit (robust to) the noise of the training data and will improve the genaralization ability of the model, 就是说限制参数，让模型少学一些noise，让模型的泛化能力更好, 往大了说，这里面有一个bias & variance 的trade off。主要有三种regularization：\n",
        "        1. L1 (lasso)\n",
        "          - this method will shrink some (less important) parameter to **ZERO**\n",
        "        2. L2 (ridge)\n",
        "        3. L1/ L2 (elastic net)\n",
        "  - Tree-based importance\n",
        "    - Tree model 有下列优点：\n",
        "      1. most popular ML algo\n",
        "      2. highly accurate\n",
        "      3. good generqalization (low oeverfitting)\n",
        "      4. robust to outlier\n",
        "      5. interpretability \n",
        "    - **Random forest importance 是按照每个feature的在N个tree中平均impurity来计算feature importance**\n",
        "    - 注意可能有坑：\n",
        "      - correlated features show equal or similar importance 说明importance高，但是可能只是correlated to other feature，所以对importance相近的feature需要double check\n",
        "      - correlated features importance is lower than the real importance\n",
        "      - **highly cardinal variables** show greater importance (trees are biased to this type of variables)\n",
        "    - Hybrid feature selection methods 一些野路子，如果你对模型要求很高\n",
        "      - feature shuffuling\n",
        "        - shuffle 每个feature，看哪个feature 被shuffle后的模型表现drop最多，说明越重要\n",
        "      - recursive feature elimination\n",
        "        - 先用tree model得出importance，然后根据importance ranking，从小到大挨个去除，看去除之后模型的performance drop情况，如果drop很多应该留着feature，如果没啥影响，那就remove吧\n",
        "      - recursive feature addition·\n",
        "        - 和上面的方法类似，先用tree model得出importance，然后根据importance ranking，从大到小挨个增加feature，看增加之后模型的performance improve的情况，如果improve很多应该留着feature，如果没啥影响，那就remove吧\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ccvihUR2Oti"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}