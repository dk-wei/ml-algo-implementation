{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CountVectorizer讲解.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNe/IZmrhfvcLWTaKqdBl2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/ml-algo-implementation/blob/main/CountVectorizer%E8%AE%B2%E8%A7%A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHTN907WVSSv"
      },
      "source": [
        "`CountVectorizer`用来对每个`document`进行`one-hot encoding`, 无论是对于NLP问题，还是多categorical feature情况，还是非常重要的。 \n",
        "\n",
        "本文主要讲两个方面:\n",
        "- `CountVectorizer`的各个parameter\n",
        "- `tokenizer`用于split各个document (避免split合成词)，以及清除punctuation\n",
        "\n",
        "我们通过不同的参数进行比较"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K0opHvnRPn9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL3-nKroL04i"
      },
      "source": [
        "# Build our text\n",
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second-document.',\n",
        "     'And this is the third-one.',\n",
        "     'Is this the first_document?',\n",
        " ]"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwHuhmBmYupv"
      },
      "source": [
        "## 默认`CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6oMZ9qIL1bS"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3On8Ac9L2aO",
        "outputId": "59a96172-80e8-4587-b769-63aca63cee16"
      },
      "source": [
        "# 我们可以看到默认的tokenizer已经清洗了每个token周围的标点\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'first_document', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jULhywpsMCSA",
        "outputId": "c8cc9237-2e91-4322-aa55-2bd9f65e8164"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 0 1 0 0 1 0 1]\n",
            " [0 2 0 0 1 0 1 1 0 1]\n",
            " [1 0 0 0 1 1 0 1 1 1]\n",
            " [0 0 0 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfhq1yGHY_on"
      },
      "source": [
        "## N-gram`CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtIeJxMkMCPZ"
      },
      "source": [
        "vectorizer2 = CountVectorizer(analyzer='word', \n",
        "                              ngram_range=(2, 2)\n",
        "                              )\n",
        "\n",
        "X2 = vectorizer2.fit_transform(corpus)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teQTvmVnMCJt",
        "outputId": "b0731857-cf38-4cb1-9ae9-d35eb7b42347"
      },
      "source": [
        "print(vectorizer2.get_feature_names())"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the first_document', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4XwP5_yMCG7",
        "outputId": "49306778-79eb-42ba-f6d3-9c671e10cadf"
      },
      "source": [
        "print(X2.toarray())"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 0 0 1 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 0 0 1 1 0 1 0]\n",
            " [0 0 0 0 1 0 0 1 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apGHl3LvZCw0"
      },
      "source": [
        "## `CountVectorizer` with new `tokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmdEN9jXMCD-"
      },
      "source": [
        "def tokenizer_splitter(s):\n",
        "  '''\n",
        "  按照space给split，再strip两边的punctuation\n",
        "  '''\n",
        "  return [i.strip(string.punctuation) for i in s.split(' ')]\n",
        "   \n",
        "vectorizer3 = CountVectorizer(analyzer='word', \n",
        "                              ngram_range=(1, 1),\n",
        "                              stop_words = ['is'],\n",
        "                              binary = False,\n",
        "                              lowercase = True,\n",
        "                              #tokenizer = lambda x: x.split(\" \"),\n",
        "                              tokenizer = tokenizer_splitter\n",
        "                              )"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvLxL2eFMBrO"
      },
      "source": [
        "X3 = vectorizer3.fit_transform(corpus)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swMnQd1JMwfI",
        "outputId": "31020595-bc0e-4ca4-975b-a29b714aed68"
      },
      "source": [
        "# 我们可以看到不会split合成词\n",
        "print(vectorizer3.get_feature_names())"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'first_document', 'second-document', 'the', 'third-one', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw9UVoswM0BT",
        "outputId": "2a8db440-0a0b-4f6f-b944-b9ee1b6e621a"
      },
      "source": [
        "print(X3.toarray())"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 0 0 1 0 1]\n",
            " [0 1 0 0 1 1 0 1]\n",
            " [1 0 0 0 0 1 1 1]\n",
            " [0 0 0 1 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOlEDYJ6M2FD",
        "outputId": "d31cab16-f732-40e8-9fc6-bf53e6b5dfdc"
      },
      "source": [
        "print(vectorizer3.vocabulary_)   # vocabulary_则是告知了每个encoding每个位置上的token情况，要好好利用"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 7, 'the': 5, 'first': 2, 'document': 1, 'second-document': 4, 'and': 0, 'third-one': 6, 'first_document': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWiKQ3ksjiCV",
        "outputId": "d5957efe-96ca-4865-db39-97d9c4c2a43e"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is the first document.',\n",
              " 'This document is the second-document.',\n",
              " 'And this is the third-one.',\n",
              " 'Is this the first_document?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsr6hYkJj0Tl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}